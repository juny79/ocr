"""
ê°„ì†Œí™”ëœ í›„ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜
Hydra configë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¡œë“œ
"""
import os
import sys
import json
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import hydra
from omegaconf import DictConfig
import lightning.pytorch as pl

sys.path.append(os.getcwd())
from ocr.lightning_modules import get_pl_modules_by_cfg

# í˜„ì¬ ìµœê³  ì ìˆ˜ì˜ íŒŒë¼ë¯¸í„°
BASELINE_THRESH = 0.231
BASELINE_BOX_THRESH = 0.432

# ê·¸ë¦¬ë“œ ì„œì¹˜ ë²”ìœ„ ì„¤ì •
THRESH_RANGE = np.arange(0.22, 0.25, 0.005)  # 0.22 ~ 0.245, step 0.005 (ì¢ì€ ë²”ìœ„)
BOX_THRESH_RANGE = np.arange(0.41, 0.45, 0.005)  # 0.41 ~ 0.445, step 0.005

CONFIG_DIR = os.environ.get('OP_CONFIG_DIR') or '../configs'


@hydra.main(config_path=CONFIG_DIR, config_name='test', version_base='1.2')
def main(config: DictConfig):
    """ê·¸ë¦¬ë“œ ì„œì¹˜ ë©”ì¸ í•¨ìˆ˜"""
    print("="*80)
    print("í›„ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜")
    print("="*80)
    print(f"Thresh range: {THRESH_RANGE[0]:.3f} ~ {THRESH_RANGE[-1]:.3f} (step: 0.005)")
    print(f"Box Thresh range: {BOX_THRESH_RANGE[0]:.3f} ~ {BOX_THRESH_RANGE[-1]:.3f} (step: 0.005)")
    print(f"Total combinations: {len(THRESH_RANGE)} Ã— {len(BOX_THRESH_RANGE)} = {len(THRESH_RANGE) * len(BOX_THRESH_RANGE)}")
    print(f"Baseline params: thresh={BASELINE_THRESH:.3f}, box_thresh={BASELINE_BOX_THRESH:.3f}")
    print()
    
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    results_dir = Path("/data/ephemeral/home/baseline_code/grid_search_results")
    results_dir.mkdir(exist_ok=True)
    results_file = results_dir / f"grid_search_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    # Checkpoint ì„¤ì •
    checkpoint_path = "outputs/hrnet_w44_1024_augmented_optimized/checkpoints/epoch=12-step=10634.ckpt"
    from omegaconf import OmegaConf
    OmegaConf.set_struct(config, False)
    config.ckpt_path = checkpoint_path
    OmegaConf.set_struct(config, True)
    
    # ê²°ê³¼ ì €ì¥ìš©
    results = {
        'baseline': {
            'thresh': BASELINE_THRESH,
            'box_thresh': BASELINE_BOX_THRESH,
            'submission_score': {
                'hmean': 0.9851,
                'precision': 0.9854,
                'recall': 0.9857
            }
        },
        'experiments': []
    }
    
    best_hmean = 0.0
    best_params = None
    
    # ê·¸ë¦¬ë“œ ì„œì¹˜
    total = len(THRESH_RANGE) * len(BOX_THRESH_RANGE)
    
    with tqdm(total=total, desc="Grid Search Progress") as pbar:
        for thresh in THRESH_RANGE:
            for box_thresh in BOX_THRESH_RANGE:
                try:
                    # í›„ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ì„¤ì •
                    from omegaconf import OmegaConf
                    OmegaConf.set_struct(config, False)
                    config.models.head.postprocess.thresh = float(thresh)
                    config.models.head.postprocess.box_thresh = float(box_thresh)
                    OmegaConf.set_struct(config, True)
                    
                    # ëª¨ë¸ ë° ë°ì´í„° ë¡œë” ìƒì„±
                    pl.seed_everything(config.get("seed", 42), workers=True)
                    model_module, data_module = get_pl_modules_by_cfg(config)
                    
                    # Trainer ìƒì„±
                    trainer = pl.Trainer(
                        logger=False,
                        enable_checkpointing=False,
                        enable_progress_bar=False,
                        enable_model_summary=False
                    )
                    
                    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                    test_results = trainer.test(
                        model_module,
                        data_module,
                        ckpt_path=checkpoint_path,
                        verbose=False
                    )
                    
                    # ê²°ê³¼ ì¶”ì¶œ
                    if test_results and len(test_results) > 0:
                        metrics = test_results[0]
                        hmean = metrics.get('test/hmean', 0.0)
                        precision = metrics.get('test/precision', 0.0)
                        recall = metrics.get('test/recall', 0.0)
                        
                        experiment_result = {
                            'thresh': float(thresh),
                            'box_thresh': float(box_thresh),
                            'metrics': {
                                'hmean': float(hmean),
                                'precision': float(precision),
                                'recall': float(recall),
                                'success': True
                            }
                        }
                        
                        results['experiments'].append(experiment_result)
                        
                        # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
                        if hmean > best_hmean:
                            best_hmean = hmean
                            best_params = {
                                'thresh': float(thresh),
                                'box_thresh': float(box_thresh),
                                'hmean': float(hmean),
                                'precision': float(precision),
                                'recall': float(recall)
                            }
                            pbar.set_postfix({'best_hmean': f"{best_hmean:.6f}"})
                    
                except Exception as e:
                    print(f"\nError at thresh={thresh:.3f}, box_thresh={box_thresh:.3f}: {e}")
                    experiment_result = {
                        'thresh': float(thresh),
                        'box_thresh': float(box_thresh),
                        'metrics': {
                            'hmean': 0.0,
                            'precision': 0.0,
                            'recall': 0.0,
                            'success': False,
                            'error': str(e)
                        }
                    }
                    results['experiments'].append(experiment_result)
                
                pbar.update(1)
                
                # ì¤‘ê°„ ì €ì¥
                if len(results['experiments']) % 20 == 0:
                    with open(results_file, 'w') as f:
                        json.dump(results, f, indent=2)
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    results['best'] = best_params
    results['completed_at'] = datetime.now().isoformat()
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print("ê·¸ë¦¬ë“œ ì„œì¹˜ ì™„ë£Œ!")
    print("="*80)
    print(f"ê²°ê³¼ íŒŒì¼: {results_file}")
    
    if best_params:
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ íŒŒë¼ë¯¸í„°:")
        print(f"  thresh: {best_params['thresh']:.4f}")
        print(f"  box_thresh: {best_params['box_thresh']:.4f}")
        print(f"  H-Mean: {best_params['hmean']:.6f}")
        print(f"  Precision: {best_params['precision']:.6f}")
        print(f"  Recall: {best_params['recall']:.6f}")
    
    # ìƒìœ„ 5ê°œ
    sorted_results = sorted(
        [r for r in results['experiments'] if r['metrics']['success']],
        key=lambda x: x['metrics']['hmean'],
        reverse=True
    )[:5]
    
    print(f"\nğŸ“ˆ ìƒìœ„ 5ê°œ ê²°ê³¼:")
    for i, result in enumerate(sorted_results, 1):
        print(f"  {i}. thresh={result['thresh']:.4f}, box_thresh={result['box_thresh']:.4f} "
              f"â†’ H-Mean: {result['metrics']['hmean']:.6f}")


if __name__ == "__main__":
    main()
