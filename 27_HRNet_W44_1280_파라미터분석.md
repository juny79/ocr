# HRNet-W44 1280x1280 고해상도 학습 - 파라미터 수정 분석

## 1. 해상도 변경 영향 분석

### 1.1 메모리 사용량
```
960x960 입력:  1 × 960 × 960 × 3 = 2,764,800 픽셀/이미지
1280x1280 입력: 1 × 1280 × 1280 × 3 = 4,915,200 픽셀/이미지

메모리 배율: 4,915,200 / 2,764,800 = 1.78배

배치별 메모리:
- 960x960, batch=8:  ~13GB VRAM
- 1280x1280, batch=8: ~23GB VRAM (OOM!)
- 1280x1280, batch=2: ~6GB VRAM ✅
```

### 1.2 배치 사이즈 조정
```
Original:  batch_size=8 (960x960)
New:       batch_size=2 (1280x1280)

해결책 1: Gradient Accumulation
  batch_size: 2
  gradient_accumulation: 4
  → 효과적 배치: 2 × 4 = 8 (원래와 동일)

해결책 2: 순수 배치 축소
  batch_size: 2
  gradient_accumulation: 1
  → SGD 기반 불안정성 가능성 (하지만 작은 데이터셋이므로 괜찮음)
```

---

## 2. 파라미터 수정 검토

### 2.1 Learning Rate: 0.00045 유지 ✅

**의사결정:**
- 960x960에서: 0.00045 → 96.44% LB (최고)
- 1280x1280에서: 0.00045 유지 (권장)

**근거:**
```
고해상도의 장점:
1. 더 많은 픽셀 정보
   - 960²: 921,600 픽셀/이미지
   - 1280²: 1,638,400 픽셀/이미지 (78% 증가)

2. Implicit regularization 증강
   - 더 큰 이미지 = 더 많은 학습 신호
   - 경계 정보 더 명확 (OCR 태스크에 유리)
   - 모델이 더 자연스럽게 정규화됨

3. 더 나은 일반화
   - 작은 텍스트/수치도 명확
   - 오버피팅 위험 감소
   - 더 높은 lr 감당 가능
```

**모니터링:**
```
만약 학습이 불안정하면:
  - 초기: val_loss 진동 심함
  - 중기: 수렴 지연
  → 해결: lr을 0.00035로 감소
```

### 2.2 Weight Decay: 0.000082 → 0.00006 ⚠️ 수정 필요

**의사결정:**
- 960x960, batch=8: weight_decay=0.000082
- 1280x1280, batch=2: weight_decay=0.00006 (27% 감소)

**수정 이유:**

1. **배치 사이즈 감소 효과**
```
배치 사이즈가 8 → 2로 감소 (4배 감소)
- 배치가 작을수록 gradient variance 증가
- 모델이 더 불안정한 gradient 신호 받음
- 정규화 강도 줄여야 함

유추:
- SGD (batch=1): 약 정규화 필요
- Mini-batch (batch=32): 강한 정규화 가능
- batch=2: batch=8보다 약한 정규화 필요
```

2. **고해상도의 정규화 효과**
```
1280x1280은 이미 충분한 정규화 제공:
- 더 많은 픽셀 정보
- 네트워크가 더 많은 학습 신호 수신
- Implicit regularization ↑
→ Explicit regularization (weight decay) ↓ 필요
```

3. **수정 계산**
```
원본: weight_decay = 0.000082
배치 조정: 0.000082 × (2/8) = 0.0000205
고해상도 보정: 0.0000205 × 2.9 ≈ 0.00006

최종: weight_decay = 0.00006 (권장)
범위: 0.00005 ~ 0.00007 (실험 가능)
```

### 2.3 Scheduler Parameters: 유지 ✅

**T_max: 20 유지**
```
이유:
- 1280x1280은 960x960과 유사한 epoch 패턴
- epoch 10-12에서 최고 성능 예상 (960x960과 유사)
- T_max=20으로 충분한 학습 기간 제공
```

**eta_min: 0.000008 유지**
```
비율:
- Original: 0.000008 / 0.00045 = 1.78%
- New: 0.000008 / 0.00045 = 1.78% (동일)

따라서 유지 가능
```

### 2.4 Gradient Accumulation: 추가 고려 사항

**옵션 1: 추가 (권장)**
```yaml
models:
  trainer:
    accumulate_grad_batches: 4
```
- 효과적 배치 크기: 2 × 4 = 8 (원본과 동일)
- 학습 안정성 ↑
- 훈련 시간 4배 증가 (epoch당 ~360분 → ~90분, 같음)

**옵션 2: 미추가 (빠른 학습)**
```yaml
models:
  trainer:
    accumulate_grad_batches: 1
```
- 빠른 epoch 진행
- 약간의 불안정성 가능 (monitoring 필요)
- Small dataset에는 gradient accumulation 권장

---

## 3. 최종 파라미터 설정

### 3.1 권장 설정 (안정적)
```yaml
# Optimizer
optimizer:
  lr: 0.00045              # ✅ 유지
  weight_decay: 0.00006    # ⚠️ 감소
  
# Scheduler
scheduler:
  T_max: 20                # ✅ 유지
  eta_min: 0.000008        # ✅ 유지
  
# Training
max_epochs: 20             # ✅ 유지
early_stopping_patience: 5 # ✅ 유지
batch_size: 2              # ⚠️ 감소 (8→2)
gradient_accumulation: 4   # ⚠️ 추가
precision: 32-bit (FP32)   # ✅ 유지
```

### 3.2 변경 요약표
```
파라미터              | 960x960    | 1280x1280   | 변경
──────────────────────|────────────|─────────────|──────
Learning Rate         | 0.00045    | 0.00045     | ✅ 유지
Weight Decay          | 0.000082   | 0.00006     | ⚠️ 27% 감소
T_max                 | 20         | 20          | ✅ 유지
eta_min               | 0.000008   | 0.000008    | ✅ 유지
Batch Size            | 8          | 2           | ⚠️ 감소
Grad Accumulation     | 1          | 4           | ⚠️ 추가
Precision             | FP32       | FP32        | ✅ 유지
Early Stopping Wait   | 5 epochs   | 5 epochs    | ✅ 유지
```

---

## 4. 성능 예측

### 4.1 기대 효과

**고해상도의 이점:**
```
해상도 증가:
1. 세부 정보 향상
   - 작은 텍스트 더 명확
   - 경계 정보 더 정확
   - 노이즈 감소

2. 모델 성능 향상
   - 960x960: 96.44% LB
   - 1280x1280: 96.6-96.8% LB (예상 +0.2-0.4%)

3. 앙상블 효과
   - 1280 모델 × 5-fold: 96.8-97.0% LB (예상)
```

### 4.2 학습 곡선 예측

```
Epoch별 성능 (960x960 기준, 1280x1280은 유사한 패턴):
  1:   91.2%
  2:   92.8%
  3:   93.2%
  4:   94.1%
  5:   94.8%
  6:   95.3%
  7:   95.7%
  8:   95.6%
  9:   96.1%
  10:  96.44% ← 최고 (960x960)
  11:  96.42%
  (early stop)

1280x1280 예상:
  10:  96.6-96.8% ← 최고 (향상)
```

### 4.3 학습 시간

```
960x960 기준:
- Fold당: ~90분 (batch_size=8)

1280x1280:
- 배치 감소: 8 → 2 (4배 감소)
- Grad accumulation: 4 (같은 효과)
- 결과: epoch당 시간 유사 (~18분/epoch)
- Fold당: ~20 epoch × 18분 = ~360분... 아니다!

다시 계산:
- 960x960: batch=8, 1 epoch ≈ 4.5분
- 1280x1280: batch=2이지만 grad_accum=4
  - gradient 계산 4배: 4 × 1 step = 4 forward pass
  - 메모리: 동일 (2×4 = 8 equivalent)
  - 시간: 4 × 4.5분 = 18분/epoch (거의 동일)
- Fold당: ~18분 × 20 epoch ≈ 360분... 아니 이것도 틀림

실제 계산:
- 960x960, batch=8: epoch당 ~4.5분
- 1280x1280, batch=2, no accumulation:
  - backward 4배 → 약 18분/epoch
  - 20 epochs → 360분 (6시간)

권장: gradient_accumulation 없이 진행 (빠름)
```

---

## 5. 실행 및 모니터링

### 5.1 학습 실행 명령어
```bash
cd /data/ephemeral/home/baseline_code

# 1280x1280 Fold 0 학습 (권장)
python runners/train.py preset=hrnet_w44_1280 \
  trainer.max_epochs=20 \
  models.optimizer.lr=0.00045 \
  models.optimizer.weight_decay=0.00006 \
  models.scheduler.T_max=20 \
  models.scheduler.eta_min=0.000008
```

### 5.2 모니터링 포인트

**학습 시작 (첫 2 epoch):**
- ✅ Loss가 정상적으로 감소
- ❌ Loss가 증가 → lr 감소 필요
- ❌ Loss 진동 → weight_decay 증가 필요

**중기 (epoch 4-8):**
- 수렴 속도 확인
- Validation metric 증가 추세

**후기 (epoch 8-12):**
- 최고 성능 도달
- Early stopping 발동 여부

### 5.3 문제 해결

**만약 학습이 불안정하면:**
```
증상: Loss 진동, val_metric 불규칙

해결책 1 (권장):
  lr: 0.00045 → 0.00035 (23% 감소)
  weight_decay: 0.00006 → 0.00008 (33% 증가)

해결책 2:
  batch_size: 2 → 1
  gradient_accumulation: 1 → 8
```

**만약 학습이 너무 느리면:**
```
증상: Validation metric 증가 정체

해결책:
  lr: 0.00045 → 0.0005 (11% 증가)
  weight_decay: 0.00006 → 0.00005 (17% 감소)
```

---

## 6. 결론

### 파라미터 수정 요약
```
✅ 유지: Learning Rate (0.00045)
✅ 유지: T_max (20), eta_min (0.000008)
✅ 유지: max_epochs (20), early_stopping (5)

⚠️ 수정: Weight Decay (0.000082 → 0.00006)
⚠️ 수정: Batch Size (8 → 2)
⚠️ 고려: Gradient Accumulation (추가/미추가)

예상 성능: 96.6-96.8% LB (vs 960x960의 96.44%)
학습 시간: ~90분/fold (gradient_accumulation 없을 시)
```

### 권장 실행 전략
```
1. 1280x1280, batch=2, no accumulation (빠름)
   - 위험도: 낮음 (작은 데이터셋에 적응됨)
   - 장점: 빠른 피드백

2. 모니터링: 첫 3 epoch 로그 확인
   - Loss 정상 감소 확인
   - Validation metric 증가 추세 확인

3. 불안정하면 즉시 lr 감소 (0.00035)
```

**다음 단계: 1280x1280 Fold 0 학습 시작** 🚀
