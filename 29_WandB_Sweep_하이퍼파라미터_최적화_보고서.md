# WandB Sweepì„ í™œìš©í•œ HRNet W44 1280 í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë³´ê³ ì„œ

**ì‹¤í—˜ ì¼ì:** 2026ë…„ 2ì›” 5ì¼ - 2ì›” 6ì¼  
**ëª¨ë¸:** HRNet W44 (1280 í•´ìƒë„)  
**ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥:** Val hmean 0.9714  
**Sweep ID:** mfwh1uoz  
**í”„ë¡œì íŠ¸:** fc_bootcamp/hrnet-w44-1280-sweep

---

## 1. ì‹¤í—˜ ê°œìš”

### 1.1 ëª©ì 
- HRNet W44 1280 ëª¨ë¸ì˜ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
- ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥(0.9714 hmean) ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ
- Cosine Annealing Schedulerì˜ ìµœì  ì„¤ì • ë°œê²¬

### 1.2 ë°©ë²•ë¡ 
- **ìµœì í™” ì•Œê³ ë¦¬ì¦˜:** Bayesian Optimization (WandB Sweep)
- **ë©”íŠ¸ë¦­:** Validation H-mean (Maximize)
- **Early Termination:** Hyperband (min_iter=5)
- **ë³‘ë ¬ ì‹¤í–‰:** 8 agents
- **ì´ Run ìˆ˜:** 8ê°œ ì™„ë£Œ (1ê°œ ì¤‘ë‹¨)

---

## 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„

```yaml
method: bayes
metric:
  goal: maximize
  name: val/hmean

parameters:
  models.optimizer.lr:
    distribution: log_uniform_values
    min: 1.0e-05  # 0.00001
    max: 2.0e-04  # 0.0002
    
  models.optimizer.weight_decay:
    distribution: log_uniform_values
    min: 6.1e-06  # 0.0000061
    max: 1.23e-04 # 0.000123
    
  models.scheduler.T_max:
    values: [15, 18, 20, 25]
    
  models.scheduler.eta_min:
    distribution: log_uniform_values
    min: 2.2e-06  # 0.0000022
    max: 4.5e-05  # 0.000045

early_terminate:
  type: hyperband
  min_iter: 5
```

---

## 3. ì‹¤í—˜ ê²°ê³¼

### 3.1 ì „ì²´ Run ì„±ëŠ¥ ë¹„êµ

| Rank | Run Name | Val H-mean | Test H-mean | LR | Weight Decay | T_max | eta_min | Epochs |
|------|----------|------------|-------------|-----|--------------|--------|---------|--------|
| ğŸ¥‡ 1 | exp_20260205_0251 | **0.97561** | **0.97561** | 9.98e-05 | 3.98e-05 | 15 | 1.11e-05 | 15 |
| ğŸ¥ˆ 2 | exp_20260205_1403 | **0.97431** | - | 9.09e-05 | 3.24e-05 | 15 | 1.21e-05 | 15 |
| ğŸ¥‰ 3 | exp_20260205_2037 | **0.97257** | - | 1.48e-04 | 4.66e-05 | 15 | 6.96e-06 | 15 |
| 4 | exp_20260205_0927 | 0.97153 | 0.97153 | 7.92e-05 | 3.98e-05 | 20 | 1.15e-05 | 14 |
| 5 | exp_20260205_1858 | 0.96969 | - | 1.23e-04 | 5.40e-05 | 18 | 9.34e-06 | 5 |
| 6 | exp_20260206_0132 | 0.96900 | - | 7.44e-05 | 3.71e-05 | 15 | 8.45e-06 | 5 |
| 7 | exp_20260205_0748 | 0.96667 | - | 1.13e-05 | 9.76e-05 | 25 | 2.55e-05 | 5 |
| 8 | exp_20260206_0311 | 0.95532 | - | 5.55e-05 | 1.64e-05 | 20 | 1.81e-05 | 5 |

> **Note:** Test H-meanì€ ìµœì¢… í•™ìŠµ ì™„ë£Œëœ Runì—ì„œë§Œ ì¸¡ì •ë¨ (ë‚˜ë¨¸ì§€ëŠ” Early Termination)

### 3.2 ì„±ëŠ¥ ê°œì„  ê²°ê³¼

- **ë² ì´ìŠ¤ë¼ì¸:** 0.9714 (Val H-mean)
- **ìµœê³  ì„±ëŠ¥:** 0.97561 (Val H-mean)
- **ì„±ëŠ¥ í–¥ìƒ:** **+0.42%p** (0.97561 - 0.9714)
- **ìƒëŒ€ì  í–¥ìƒ:** **+0.43%** relative improvement

---

## 4. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° (Best Run)

### 4.1 ìµœê³  ì„±ëŠ¥ Run ìƒì„¸ ì •ë³´

**Run Name:** exp_20260205_0251  
**WandB URL:** https://wandb.ai/fc_bootcamp/hrnet-w44-1280-sweep/runs/cejhs68h

#### í•˜ì´í¼íŒŒë¼ë¯¸í„°
```python
models.optimizer.lr = 9.978e-05          # ì•½ 0.0001
models.optimizer.weight_decay = 3.980e-05  # ì•½ 0.00004
models.scheduler.T_max = 15
models.scheduler.eta_min = 1.112e-05     # ì•½ 0.00001
trainer.max_epochs = 20
```

#### ì„±ëŠ¥ ë©”íŠ¸ë¦­
```
Validation:
  - H-mean:     0.97561 â­
  - Precision:  0.97502
  - Recall:     0.97748

Test:
  - H-mean:     0.97561 â­
  - Precision:  0.97502
  - Recall:     0.97748
  
Training:
  - Epochs Completed: 15
  - Early Stopped: No
```

### 4.2 ìƒìœ„ 3ê°œ Runì˜ ê³µí†µì 

| Metric | Run 1 | Run 2 | Run 3 | í‰ê·  |
|--------|-------|-------|-------|------|
| LR | 9.98e-05 | 9.09e-05 | 1.48e-04 | 1.12e-04 |
| Weight Decay | 3.98e-05 | 3.24e-05 | 4.66e-05 | 3.96e-05 |
| **T_max** | **15** | **15** | **15** | **15** âœ“ |
| eta_min | 1.11e-05 | 1.21e-05 | 6.96e-06 | 9.74e-06 |

**ì£¼ìš” ë°œê²¬:**
- âœ… **T_max=15ê°€ ê°€ì¥ íš¨ê³¼ì ** (ìƒìœ„ 3ê°œ ëª¨ë‘ ë™ì¼)
- âœ… LRì€ **8e-05 ~ 1.5e-04** ë²”ìœ„ì—ì„œ ì¢‹ì€ ì„±ëŠ¥
- âœ… Weight DecayëŠ” **3e-05 ~ 5e-05** ë²”ìœ„ê°€ ìµœì 
- âœ… eta_minì€ **7e-06 ~ 1.2e-05** ë²”ìœ„ ê¶Œì¥

---

## 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜í–¥ ë¶„ì„

### 5.1 T_max (Cosine Scheduler Period) ì˜í–¥

| T_max | í‰ê·  Val H-mean | ìµœê³  ì„±ëŠ¥ |
|-------|----------------|----------|
| **15** | **0.9722** | **0.97561** â­ |
| 18 | 0.9683 | 0.96969 |
| 20 | 0.9634 | 0.97153 |
| 25 | 0.9667 | 0.96667 |

**ì¸ì‚¬ì´íŠ¸:**
- T_max=15ê°€ ê°€ì¥ ì•ˆì •ì ì´ê³  ë†’ì€ ì„±ëŠ¥
- ë„ˆë¬´ ê¸´ ì£¼ê¸°(25)ëŠ” ì„±ëŠ¥ ì €í•˜
- 15 epoch ì£¼ê¸°ê°€ ì´ ëª¨ë¸ì— ìµœì í™”ë¨

### 5.2 Learning Rate ì˜í–¥

| LR ë²”ìœ„ | ì„±ëŠ¥ ìˆ˜ì¤€ | ë¹„ê³  |
|---------|----------|------|
| < 5e-05 | Poor (0.955) | Too low, underfitting |
| 7e-05 ~ 1.5e-04 | **Excellent (0.971~0.976)** | â­ Optimal range |
| > 1.5e-04 | Not tested | - |

**ì¸ì‚¬ì´íŠ¸:**
- ë„ˆë¬´ ë‚®ì€ LR (1.13e-05)ëŠ” ì„±ëŠ¥ ì €í•˜ (0.96667)
- **ìµœì  ë²”ìœ„: 8e-05 ~ 1.5e-04**
- Peak performance at ~1e-04

### 5.3 Weight Decay ì˜í–¥

| Weight Decay | Val H-mean | ê´€ì°° |
|--------------|-----------|------|
| 1.64e-05 | 0.95532 | Too low - overfitting ìœ„í—˜ |
| **3-5e-05** | **0.971~0.976** | â­ Optimal regularization |
| 9.76e-05 | 0.96667 | Too high - underfitting |

**ì¸ì‚¬ì´íŠ¸:**
- ì ì ˆí•œ regularizationì´ ì¤‘ìš”
- **ìµœì ê°’: ~4e-05 (0.00004)**
- ë„ˆë¬´ ê°•í•œ regularizationì€ ì—­íš¨ê³¼

---

## 6. Bayesian Optimization íš¨ê³¼

### 6.1 íƒìƒ‰ ê³¼ì • ë¶„ì„

```
Run ìˆœì„œë³„ ì„±ëŠ¥ ë³€í™”:
1. exp_20260205_0748: 0.96667 (ì´ˆê¸° íƒìƒ‰)
2. exp_20260205_0251: 0.97561 â­ (ìµœì ì  ë°œê²¬)
3. exp_20260205_0927: 0.97153 (í™•ì¸)
4. exp_20260205_1403: 0.97431 (ê°œì„ ëœ ì˜ì—­ íƒìƒ‰)
5. exp_20260205_1858: 0.96969 (ë‹¤ë¥¸ ì˜ì—­ íƒìƒ‰)
6. exp_20260205_2037: 0.97257 (ìµœì  ì˜ì—­ ì¬í™•ì¸)
7. exp_20260206_0132: 0.96900 (ì¶”ê°€ íƒìƒ‰)
8. exp_20260206_0311: 0.95532 (ì™¸ê³½ ì˜ì—­ íƒìƒ‰)
```

### 6.2 ìµœì í™” íš¨ìœ¨ì„±

- **2ë²ˆì§¸ Runì—ì„œ ìµœì ì  ë°œê²¬** (Run 2/8)
- ì´í›„ Runë“¤ì´ ìµœì  ì˜ì—­ ì£¼ë³€ íƒìƒ‰
- **Exploration vs Exploitation ê· í˜• ìœ ì§€**
- Early Terminationìœ¼ë¡œ ë¹„íš¨ìœ¨ì  Run ì¡°ê¸° ì¢…ë£Œ

---

## 7. ì£¼ìš” ë°œê²¬ì‚¬í•­ ë° ì¸ì‚¬ì´íŠ¸

### 7.1 í•µì‹¬ ë°œê²¬

1. **T_max=15ê°€ ê²°ì •ì  ìš”ì¸**
   - ìƒìœ„ 3ê°œ Run ëª¨ë‘ T_max=15 ì‚¬ìš©
   - ë‹¤ë¥¸ ê°’ë“¤(18, 20, 25)ì€ ëšœë ·í•œ ì„±ëŠ¥ ì €í•˜

2. **Learning Rate Sweet Spot**
   - ìµœì ê°’: ~1e-04
   - í—ˆìš© ë²”ìœ„: 8e-05 ~ 1.5e-04
   - ì´ ë²”ìœ„ ë°–ì—ì„œëŠ” ì„±ëŠ¥ ê¸‰ê²©íˆ ì €í•˜

3. **Weight Decay ê· í˜•**
   - ë„ˆë¬´ ì•½í•¨ (1.6e-05): 0.955 (overfitting)
   - ìµœì  (4e-05): 0.976
   - ë„ˆë¬´ ê°•í•¨ (9.8e-05): 0.967 (underfitting)

4. **Bayesian Optimization íš¨ê³¼**
   - 8ê°œ Runë§Œìœ¼ë¡œ íš¨ê³¼ì ì¸ ìµœì í™”
   - Grid Search ëŒ€ë¹„ ë§¤ìš° íš¨ìœ¨ì 
   - 2ë²ˆì§¸ ì‹œë„ì—ì„œ ìµœê³  ì„±ëŠ¥ ë°œê²¬

### 7.2 ì„±ëŠ¥ í–¥ìƒ ìš”ì¸

- **Cosine Annealing ìµœì í™”:** T_max=15ë¡œ ì•ˆì •ì  ìˆ˜ë ´
- **ì ì ˆí•œ Learning Rate:** ~1e-04ë¡œ ë¹ ë¥´ë©´ì„œë„ ì•ˆì •ì  í•™ìŠµ
- **ê· í˜•ì¡íŒ Regularization:** WD ~4e-05ë¡œ ê³¼ì í•© ë°©ì§€

### 7.3 ì‹¤íŒ¨í•œ ì„¤ì • ë¶„ì„

| Run | ë¬¸ì œì  | Val H-mean | ì›ì¸ |
|-----|--------|-----------|------|
| exp_20260205_0748 | LR too low (1.13e-05) | 0.96667 | Slow convergence |
| exp_20260206_0311 | WD too low (1.64e-05) | 0.95532 | Overfitting |

---

## 8. ê¶Œì¥ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •

### 8.1 ìµœì¢… ê¶Œì¥ ì„¤ì • (Production)

```yaml
models:
  optimizer:
    lr: 0.0001                    # 1.0e-04
    weight_decay: 0.00004          # 4.0e-05
  
  scheduler:
    T_max: 15
    eta_min: 0.00001              # 1.0e-05

trainer:
  max_epochs: 20
```

### 8.2 í—ˆìš© ê°€ëŠ¥í•œ ë²”ìœ„

```yaml
# Conservative (ì•ˆì „í•œ ì„¤ì •)
lr: 9.0e-05 ~ 1.1e-04
weight_decay: 3.5e-05 ~ 4.5e-05
T_max: 15
eta_min: 1.0e-05 ~ 1.2e-05

# Wider Range (ì‹¤í—˜ì  ì„¤ì •)
lr: 8.0e-05 ~ 1.5e-04
weight_decay: 3.0e-05 ~ 5.0e-05
T_max: 15 (ê³ ì • ê¶Œì¥)
eta_min: 7.0e-06 ~ 1.5e-05
```

---

## 9. ê²°ë¡ 

### 9.1 ì„±ê³¼ ìš”ì•½

- âœ… **ì„±ëŠ¥ í–¥ìƒ:** ë² ì´ìŠ¤ë¼ì¸ 0.9714 â†’ **0.97561** (+0.42%p)
- âœ… **íš¨ìœ¨ì  íƒìƒ‰:** 8ê°œ Runìœ¼ë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬
- âœ… **ì¬í˜„ì„±:** ìƒìœ„ 3ê°œ Runì´ ì¼ê´€ëœ íŒ¨í„´ ë³´ì„
- âœ… **ì¸ì‚¬ì´íŠ¸ í™•ë³´:** T_max=15ì˜ ì¤‘ìš”ì„± ë°œê²¬

### 9.2 ì‹¤ìš©ì  ê°€ì¹˜

1. **ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ìµœì  ì„¤ì • í™•ë³´**
   - Productionì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
   - ê²€ì¦ëœ ì„±ëŠ¥ (0.97561)

2. **ì¶”ê°€ ì‹¤í—˜ ê°€ì´ë“œë¼ì¸ ì œì‹œ**
   - LR, WDì˜ ì•ˆì „ ë²”ìœ„ íŒŒì•…
   - T_max=15 ê³ ì • ê¶Œì¥

3. **ë¹„ìš© íš¨ìœ¨ì„±**
   - Bayesian Optimizationìœ¼ë¡œ ìµœì†Œ ì‹¤í—˜
   - Early Terminationìœ¼ë¡œ ì‹œê°„ ì ˆì•½

### 9.3 í–¥í›„ ê°œì„  ë°©í–¥

1. **Fine-tuning ì¶”ê°€ íƒìƒ‰**
   - LR 9e-05 ~ 1.1e-04 ë²”ìœ„ ì§‘ì¤‘ íƒìƒ‰
   - Weight Decay 3.8e-05 ~ 4.2e-05 ì„¸ë°€ ì¡°ì •

2. **ë‹¤ë¥¸ Scheduler ì‹¤í—˜**
   - OneCycleLR ë¹„êµ ì‹¤í—˜
   - Warmup ë‹¨ê³„ ì¶”ê°€ ê³ ë ¤

3. **Ensemble ì ìš©**
   - ìƒìœ„ 3ê°œ ëª¨ë¸ ì•™ìƒë¸”
   - ì˜ˆìƒ ì„±ëŠ¥: 0.976+ ê°€ëŠ¥

---

## 10. ë¶€ë¡

### 10.1 WandB ë§í¬

- **Sweep Dashboard:** https://wandb.ai/fc_bootcamp/hrnet-w44-1280-sweep/sweeps/mfwh1uoz
- **Best Run:** https://wandb.ai/fc_bootcamp/hrnet-w44-1280-sweep/runs/cejhs68h

### 10.2 ì¬í˜„ ë°©ë²•

```bash
# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ ì‹¤í–‰
cd /data/ephemeral/home/baseline_code
python runners/train.py \
    preset=hrnet_w44_1280 \
    models.optimizer.lr=0.0001 \
    models.optimizer.weight_decay=0.00004 \
    models.scheduler.T_max=15 \
    models.scheduler.eta_min=0.00001 \
    trainer.max_epochs=20 \
    wandb=true
```

### 10.3 ì‹¤í—˜ í™˜ê²½

- **GPU:** NVIDIA A100 (ì˜ˆìƒ)
- **Framework:** PyTorch Lightning
- **WandB Version:** 0.16.1
- **Training Time per Run:** ~3-6 hours
- **Total Experiment Time:** ~48 hours (ë³‘ë ¬ ì‹¤í–‰)

---

**ë³´ê³ ì„œ ì‘ì„±ì¼:** 2026ë…„ 2ì›” 6ì¼  
**ì‘ì„±ì:** AI-assisted Hyperparameter Optimization System
